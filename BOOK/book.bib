@article{friedman2008predictive,
  title={Predictive learning via rule ensembles},
  author={Friedman, Jerome H and Popescu, Bogdan E},
  journal={The Annals of Applied Statistics},
  pages={916--954},
  year={2008},
  publisher={JSTOR}
}

@article{bike2013,
year={2013},
issn={2192-6352},
journal={Progress in Artificial Intelligence},
doi={10.1007/s13748-013-0040-3},
title={Event labeling combining ensemble detectors and background knowledge},
url={[Web Link]},
publisher={Springer Berlin Heidelberg},
keywords={Event labeling; Event detection; Ensemble learning; Background knowledge},
author={Fanaee-T, Hadi and Gama, Joao},
pages={1-15}
}

@inproceedings{fernandes2017transfer,
  title={Transfer Learning with Partial Observability Applied to Cervical Cancer Screening},
  author={Fernandes, Kelwin and Cardoso, Jaime S and Fernandes, Jessica},
  booktitle={Iberian Conference on Pattern Recognition and Image Analysis},
  pages={243--250},
  year={2017},
  organization={Springer}
}

@article{Strobl2008,
abstract = {BACKGROUND: Random forests are becoming increasingly popular in many scientific fields because they can cope with "small n large p" problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables. RESULTS: We identify two mechanisms responsible for this finding: (i) A preference for the selection of correlated predictors in the tree building process and (ii) an additional advantage for correlated predictor variables induced by the unconditional permutation scheme that is employed in the computation of the variable importance measure. Based on these considerations we develop a new, conditional permutation scheme for the computation of the variable importance measure. CONCLUSION: The resulting conditional variable importance reflects the true impact of each predictor variable more reliably than the original marginal approach.},
author = {Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
doi = {10.1186/1471-2105-9-307},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Strobl et al. - 2008 - Conditional variable importance for random forests.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Amino Acid Sequence,Binding Sites,Biometry,Biometry: methods,Computational Biology,Computational Biology: methods,Decision Trees,Factor Analysis,Major Histocompatibility Complex,Major Histocompatibility Complex: genetics,Nonparametric,Regression Analysis,Research Design,Statistical,Statistics,xai-book},
mendeley-groups = {Master Thesis},
mendeley-tags = {xai-book},
month = {jan},
pages = {307},
pmid = {18620558},
title = {{Conditional variable importance for random forests.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2491635{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {9},
year = {2008}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{goldstein2015peeking,
  title={Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation},
  author={Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  journal={Journal of Computational and Graphical Statistics},
  volume={24},
  number={1},
  pages={44--65},
  year={2015},
  publisher={Taylor \& Francis}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@inproceedings{alberto2015tubespam,
  title={Tubespam: Comment spam filtering on youtube},
  author={Alberto, T{\'u}lio C and Lochter, Johannes V and Almeida, Tiago A},
  booktitle={Machine Learning and Applications (ICMLA), 2015 IEEE 14th International Conference on},
  pages={138--143},
  year={2015},
  organization={IEEE}
}


@article{Lipton2016,
abstract = {Supervised machine learning models boast re-markable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but inter-pretable. And yet the task of interpretation ap-pears underspecified. Papers provide diverse and sometimes non-overlapping motivations for in-terpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim inter-pretability axiomatically, absent further explana-tion. In this paper, we seek to refine the dis-course on interpretability. First, we examine the motivations underlying interest in interpretabil-ity, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of dif-ferent notions, and question the oft-made asser-tions that linear models are interpretable and that deep neural networks are not.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.03490v1},
author = {Lipton, Zachary C},
eprint = {arXiv:1606.03490v1},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Lipton - 2016 - The Mythos of Model Interpretability.pdf:pdf},
journal = {ICML Workshop on Human Interpretability in Machine Learning},
keywords = {Black Box,Deep Learning,Interpretability,Machine Learning,Supervised Learning,xai-book},
mendeley-tags = {xai-book},
number = {Whi},
title = {{The Mythos of Model Interpretability}},
year = {2016}
}

@book{Hastie2009,
author = {Hastie, T and Tibshirani, R and Friedman, J},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - 2009 - The elements of statistical learning.pdf:pdf},
title = {{The elements of statistical learning}},
url = {http://link.springer.com/content/pdf/10.1007/978-0-387-84858-7.pdf},
year = {2009}
}



@article{Ribeiro2016b,
abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found re-newed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to inter-pretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, com-parison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
archivePrefix = {arXiv},
arxivId = {1606.05386},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1606.05386},
file = {:Users/chris/Downloads/1606.05386.pdf:pdf},
journal = {ICML Workshop on Human Interpretability in Machine Learning},
keywords = {comprehensibil,interpretability, machine learning, comprehensibil,machine learning,model-agnostic,xai-book},
mendeley-tags = {model-agnostic,xai-book},
number = {Whi},
title = {{Model-Agnostic Interpretability of Machine Learning}},
year = {2016}
}

@article{Doshi-Velez2017,
abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
archivePrefix = {arXiv},
arxivId = {1702.08608},
author = {Doshi-Velez, Finale and Kim, Been},
eprint = {1702.08608},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Doshi-Velez, Kim - 2017 - Towards A Rigorous Science of Interpretable Machine Learning.pdf:pdf},
keywords = {xai-book},
mendeley-tags = {xai-book},
number = {Ml},
pages = {1--13},
title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
url = {http://arxiv.org/abs/1702.08608},
year = {2017}
}

@misc{algorithm,
  title = {Definition of Algorithm},
  howpublished = {\url{https://www.merriam-webster.com/dictionary/algorithm}},
  note = {Accessed: 2017-02-12},
  year = {2017}
}


@article{Turner2015,
abstract = {We propose a general model explanation system (MES) for “explaining” the output of black box classifiers. In this introduction we use the motivating example of a classifier trained to detect fraud in a credit card transaction history. The key aspect is that we provide explanations applicable to a single prediction, rather than provide an interpretable set of parameters. The labels in the provided examples are usually negative. Hence, we focus on explaining positive predictions (alerts).},
annote = {From Duplicate 2 (A Model Explanation System - Turner, Ryan) Extensions to multi-class should be easy, because if you only look at one class of interest it is again the same as the binary case. How to extend this to regression?},
archivePrefix = {arXiv},
arxivId = {1606.09517},
author = {Turner, Ryan},
eprint = {1606.09517},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Turner - Unknown - A Model Explanation System.pdf:pdf;:Users/chris/Downloads/1606.09517.pdf:pdf},
isbn = {9781509007462},
journal = {NIPS Workshop},
keywords = {,xai-book},
mendeley-tags = {xai-book},
pages = {1--5},
title = {{A Model Explanation System}},
url = {http://www.blackboxworkshop.org/pdf/Turner2015{\_}MES.pdf},
volume = {0},
year = {2015}
}

@article{miller2017explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={arXiv preprint arXiv:1706.07269},
  year={2017}
}

@article{lipton1990contrastive,
  title={Contrastive explanation},
  author={Lipton, Peter},
  journal={Royal Institute of Philosophy Supplements},
  volume={27},
  pages={247--266},
  year={1990},
  publisher={Cambridge University Press}
}

@article{heider1944experimental,
  title={An experimental study of apparent behavior},
  author={Heider, Fritz and Simmel, Marianne},
  journal={The American journal of psychology},
  volume={57},
  number={2},
  pages={243--259},
  year={1944},
  publisher={JSTOR}
}


@techreport{kahneman1981simulation,
  title={The simulation heuristic.},
  author={Kahneman, Daniel and Tversky, Amos},
  year={1981},
  institution={STANFORD UNIV CA DEPT OF PSYCHOLOGY}
}

@misc{grice1975logic,
  title={Logic and Conversation: In Syntax and Seman-tics3},
  author={Grice, HP},
  year={1975},
  publisher={New York: New York Press}
}


@inproceedings{vstrumbelj2011general,
  title={A general method for visualizing and explaining black-box regression models},
  author={{\v{S}}trumbelj, Erik and Kononenko, Igor},
  booktitle={International Conference on Adaptive and Natural Computing Algorithms},
  pages={21--30},
  year={2011},
  organization={Springer}
}


@article{nickerson1998confirmation,
  title={Confirmation bias: A ubiquitous phenomenon in many guises.},
  author={Nickerson, Raymond S},
  journal={Review of general psychology},
  volume={2},
  number={2},
  pages={175},
  year={1998},
  publisher={Educational Publishing Foundation}
}

@Manual{pre2017,
  title = {pre: Prediction Rule Ensembles},
  author = {Marjolein Fokkema and Benjamin Christoffersen},
  year = {2017},
  note = {R package version 0.4},
  url = {https://CRAN.R-project.org/package=pre},
}

@article{Lundberg2017,
abstract = {Understanding why a model made a certain prediction is crucial in many applications. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, such as ensemble or deep learning models. This creates a tension between accuracy and interpretability. In response, a variety of methods have recently been proposed to help users interpret the predictions of complex models. Here, we present a unified framework for interpreting predictions, namely SHAP (SHapley Additive exPlanations, which assigns each feature an importance for a particular prediction. The key novel components of the SHAP framework are the identification of a class of additive feature importance measures and theoretical results that there is a unique solution in this class with a set of desired properties. This class unifies six existing methods, and several recent methods in this class do not have these desired properties. This means that our framework can inform the development of new methods for explaining prediction models. We demonstrate that several new methods we presented in this paper based on the SHAP framework show better computational performance and better consistency with human intuition than existing methods.},
annote = {A big issue with shapley value: The attribution of feature effects for f() is always compared to a reference instance (implicitly). The reference instance is one were all features are missing and that's were the problems start. For tabular data it's basically that you have to binarize your features and the instance for which you want to have an explanation of the prediction has all 1's in the feature vector. The feature vector is 0 if all features are different from the instance of interest. So basically the 0 or empty instance is the average of instances that are different from the instance of interest. But I find that a bit tricky: How do you binarize the features? Choosing the empty instance makes all the difference in explaining an instances prediction.

No paper really explains what it means to have a feature contribution. So you have a rent prediction of 1000€. Do you want to know the contribution in € for each feature compared to 0€? Or compared to the average prediction? Or compared to the average prediction for all the instances that are different from the instance of interest (which is the shapley approach)?},
archivePrefix = {arXiv},
arxivId = {1705.07874},
author = {Lundberg, Scott and Lee, Su-In},
eprint = {1705.07874},
file = {:Users/chris/Downloads-paper/1705.07874.pdf:pdf},
journal = {arXiv preprint arXiv},
mendeley-groups = {IML/5. done},
number = {Section 3},
title = {{A unified approach to interpreting model predictions}},
url = {http://arxiv.org/abs/1705.07874},
year = {2017}
}

@article{strumbelj2014,
abstract = {We present a sensitivity analysis-based method for explaining prediction models that can be applied to any type of classification or regression model. Its advantage over existing general methods is that all subsets of input features are perturbed, so interactions and redundancies between features are taken into account. Furthermore, when explaining an additive model, the method is equivalent to commonly used additive model-specific methods. We illustrate the method's usefulness with examples from artificial and real-world data sets and an empirical analysis of running times. Results from a controlled experiment with 122 participants suggest that the method's explanations improved the participants' understanding of the model.},
author = {Strumbelj, Erik and Kononenko, Igor and {\v{S}}trumbelj, Erik and Kononenko, Igor},
doi = {10.1007/s10115-013-0679-x},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/{\v{S}}trumbelj, Kononenko - 2014 - Explaining prediction models and individual predictions with feature contributions.pdf:pdf;:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Strumbelj et al. - 2014 - Explaining prediction models and individual predictions with feature contributions.pdf:pdf},
isbn = {0219-1377},
issn = {02193116},
journal = {Knowledge and Information Systems},
keywords = {Data mining,Decision support,Interpretability,Knowledge discovery,Visualization},
mendeley-groups = {IML/5. done},
number = {3},
pages = {647--665},
title = {{Explaining prediction models and individual predictions with feature contributions}},
volume = {41},
year = {2014}
}

@article{shapley1953value,
  title={A value for n-person games},
  author={Shapley, Lloyd S},
  journal={Contributions to the Theory of Games},
  volume={2},
  number={28},
  pages={307--317},
  year={1953}
}

@article{Lundberg2016,
abstract = {Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.},
archivePrefix = {arXiv},
arxivId = {1611.07478},
author = {Lundberg, Scott and Lee, Su-In},
eprint = {1611.07478},
file = {:Users/chris/Downloads-paper/1611.07478.pdf:pdf},
mendeley-groups = {IML/5. done},
number = {Nips},
pages = {1--6},
title = {{An unexpected unity among methods for interpreting model predictions}},
url = {http://arxiv.org/abs/1611.07478},
year = {2016}
}


@article{Fisher2018,
abstract = {There are serious drawbacks to many current variable importance (VI) methods, in that they tend to not be comparable across model types, can obscure implicit assumptions about the data generating distribution, or can give seemingly incoherent results when multiple prediction models fit the data well. In this paper we propose a framework of VI measures for describing how much any model class (e.g. all linear models of dimension p), any model-fitting algorithm (e.g. Ridge regression with fixed regularization parameter), or any individual prediction model (e.g. a single linear model with fixed coefficient vector), relies on covariate(s) of interest. The building block of our approach, Model Reliance (MR), compares a prediction model's expected loss with that model's expected loss on a pair observations in which the value of the covariate of interest has been switched. Expanding on MR, we propose Model Class Reliance (MCR) as the upper and lower bounds on the degree to which any well-performing prediction model within a class may rely on a variable of interest, or set of variables of interest. Thus, MCR describes reliance on a variable while accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. We give probabilistic bounds for MR and MCR, leveraging existing results for U-statistics. We also illustrate connections between MR, conditional causal effects, and linear regression coefficients. We outline implementations of our approaches for regularized linear regression, and to regression in a reproducing kernel Hilbert space. We then apply MR {\&} MCR to study the behavior of recidivism prediction models, using a public dataset of Broward County criminal records.},
archivePrefix = {arXiv},
arxivId = {1801.01489},
author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
eprint = {1801.01489},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Fisher, Rudin, Dominici - 2018 - Model Class Reliance Variable Importance Measures for any Machine Learning Model Class, from the Rashom.pdf:pdf},
title = {{Model Class Reliance: Variable Importance Measures for any Machine Learning Model Class, from the "Rashomon" Perspective}},
url = {http://arxiv.org/abs/1801.01489},
year = {2018}
}

