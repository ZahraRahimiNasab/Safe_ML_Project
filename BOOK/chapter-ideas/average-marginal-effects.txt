### Average Marginal Effects}

Average marginal effects were originally developed for generalized linear models with special emphasis on the logistic regression model (CITATION).
Logistic regression models try to model the probability of a binary response using a set of features.
The main idea in generalized linear models is to model the expected value of the target feature using a linear combination (also called linear predictor) of the features by weighting them with the coefficients $\beta$ and a so-called response function $h$, i.e.
$$E(Y|X) = h(x^\top \beta).$$
In any case the linear predictor takes values between minus and plus infinity, i.e., for a logistic regression model those values need to be translated into probabilities which is typically done by appling a sigmoid function (e.g., using a logistic function for $h$) to the linear predictor to make sure that the values are between 0 and 1.
Although the logistic regression model is claimed to be an interpretable model (CITATION), the interpretations of the estimated coefficients (feature effects) are not directly related to the probability of the binary response due to the non-linear response function $h$.
Instead, the interpretations are done w.r.t. log-odds, e.g. if $x$ increases by one, the log-odds ($log \frac{P(Y = 1)}{P(Y = 0)}$) will increase by $\beta$.
Researchers of applied sciences, however, are often interested in making direct interpretations of how changes in $x$ affect the probability $P(Y=1)$. For this purpose one can use average marginal effects.
