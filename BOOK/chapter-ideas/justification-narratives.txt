## Justification narrative structure for classification {#justificationNarrative}
See also Paper.
Idea: Per feature only use effect and importance. This method is per se model-agnostic, but you need a method for computing effect and importance, which is different for each model class.

The **effect** of a feature is how much the feature contributed towards (or against) a classification to a certain category for an instance. In case of a linear model it is simply the j-th weight times the feature value for observation i: $\beta_{j} x_{ij}$. For classification it is class specific (class k):  $eff_{ji} = \beta_{kj} x_{ij}$

The **importance** of a feature is defined as the overall strength of a feature within the model. So it is the expected effect of feature j for a particular class. The formula for importance of feature j towards class k is: $imp_{ji} = \beta_{ji} \frac{\sum_{x \in X^j} x_{i}}{|X^j|}$, where $X^j$ is the set of all instances which have class j. Note that the polarity of a feature ($=sign(\beta_{j})$) might be different from the importance, for example when the weight is negative and also the associated feature is negative for most cases in class k.


**Narrative role** of a feature for the classification of an instance depends on effect and importance.

Step 1: Decide what magnitude of importance can be seen as high and separate into low and high. This can be done by applying a fixed threshold or keeping a fixed number of features or some kind of 'ellbow criterium'.  The absolute magnitude has to be considered because importance comes both from features that count towards and against a class.


| Importance \ Effect | High positive | Low |  High negative |
|:--------------|:--------------------|:------------------------|:----------------------------|
|High positive |Normal evidence     |Missing evidence        |Contrarian counter-evidence |
|Low           |Exceptional evidence|Negligible              |Exceptional counter-evidence|
|High negative |Contrarian evidence |Missing counter-evidence|Normal counter-evidence     |


Contrarian evidene and contrarian counter-evidence is only possible with negative features.


You should mean center the features, otherwise the importance and the effects will very much look the same (unless the means between the classes vary greatly). The importance and effects are dependent on the scale of your features, but it should not matter whether the a feature is measured in meters or in inch (you should use meter of course) or if it is visits per hour or per minute.

Textual template: TODO

### Example justification narratives with the vehicle data set
The Vehicle dastaset contains the silhoutte descriptions of four types of vehicles. Different features are extracted from the silhouettes from different angles. The four classes are bus, opel, saab and van, but for the purpose of illustration we only focus on the task classifying bus vs. not bus given the silhoutte features. The dataset contains 846 cars with 18 silhoutte features.
```{r justification_narratives}


## Create data set for classification and train classifier
data('Vehicle')
X = Vehicle
normalize = function(x){(x - mean(x)) / sd(x)}
X = X %>% mutate_each(funs(normalize), -Class)
## Task: Bus vs. not bus
X$Class = ifelse(X$Class == 'bus', 1, 0)
classifier = glm(Class ~ ., data = X,  family = binomial())
y = as.factor(X$Class)
X$Class = NULL

feature_descriptions = c('Compactness', 'Circularity', 'Distance Circularity', 	'Radius ratio',
'pr.axis aspect ratio', 'max.length aspect ratio', 'scatter ratio', 'elongatedness', 'pr.axis rectangularity', 'max.length rectangularity', 'scaled variance along major axis', 'scaled variance along minor axis', 'scaled radius of gyration', 'skewness about major axis', 'skewness about minor axis', 	'kurtosis about minor axis', '	kurtosis about major axis', 	'hollows ratio')

#' Calculate importance of features
#'
#' See also  Justification Narratives for Individual Classifications
#'
#' @param X The data.frame with the features. Have to be named the same as in model
#' @param mod The logistic regression model of class 'glm'
#' @param y The vector with the class variable
#' @param class The name of the class for which the importance will be computed
importance_binom = function(X, mod, y, class){
  feature_names = names(X)
  coefs = coefficients(mod)[feature_names]
  X_class = X[y == class, ]
  coefs * (colSums(X_class[feature_names])/nrow(X_class))
}

imps = importance_binom(X, classifier,  y, 1)
imps = data.frame(imps)
names(imps) = 'importance'
imps$coef = rownames(imps)

imps_print = imps
imps_print$feature_descriptions = feature_descriptions
imps_print = rename(imps_print, Feature=coef, Description=feature_descriptions, Importance=importance)
kable(imps_print[c('Feature', 'Description', 'Importance')], caption = 'Feature importances for the class "bus"', row.names = FALSE)
## ggplot(imps) + geom_segment(aes(x = coef, xend = coef, y = 0, yend = importance)) + ggtitle('Importance per variable')

important_features = imps[abs(imps$importance) > 1,]

#' Calculate effect of features for single instance
#'
#' See also  Justification Narratives for Individual Classifications
#'
#' @param X The data.frame with the features. Have to be named the same as in model
#' @param mod The logistic regression model of class 'glm'
#' @param i The index of the observation for which to compute the effect
effects_binom = function(X, i, mod){
  instance = X[i,]
  feature_names = names(X)
  coefs = mod$coefficients[feature_names]
  coefs * instance
}

## Compute effects for all instances
effects = lapply(1:nrow(X),  function(x){effects_binom(X, x, classifier)})
effects_r = data.table::rbindlist(effects)

## Choose exemplary instances
instance_indices = c(1, 5)

instance_index1 = 1
instance_index2 = 5

pdata = gather(effects_r[instance_index1,])
pdata$type = 'effect'

pdata_imps = imps %>% rename(key=coef, value=importance) %>%
  mutate(type = 'importance')

ggplot() +
  geom_segment(aes(y = key, yend = key, x = 0, xend = value), data = pdata_imps) +
  geom_point(aes(y = key, x = value), data = pdata) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + guides(lty = guide_legend())


ggplot() +  geom_bar(aes(x = key,  y= value, fill=type, lty=type), data = rbind(pdata_imps, pdata), stat='identity', alpha = 0.3, position= position_identity()) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
